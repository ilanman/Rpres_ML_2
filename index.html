<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning with R - Part II</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning with R - Part II">
  <meta name="author" content="Ilan Man">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Machine Learning with R - Part II</h1>
        <h2></h2>
        <p>Ilan Man<br/>Strategy Operations  @ Squarespace</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Objectives</h2>
  </hgroup>
  <article>
    <p><space></p>

<ul>
<li>Better understand popular ML algorithms and techniques</li>
<li>Less code, more insight</li>
<li>Familiarity with basic statistics concepts assumed</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Logistic Regression</li>
<li>Principal Component Analysis</li>
<li>Clustering</li>
<li>Trees</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Objectives</h1>

<p><space></p>

<ol>
<li>Motivation</li>
<li>Concepts and key assumptions</li>
<li>Popular approximation approach</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit.png" alt="plot of chunk log_bad_fit"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit2.png" alt="plot of chunk log_bad_fit2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit3.png" alt="plot of chunk log_bad_fit3"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Binary response variable (Y = 1 or Y = 0) association to a set of explanatory variables</li>
<li>Like Linear Regression with a categorical outcome</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Binary response variable (Y = 1 or Y = 0) association to a set of explanatory variables</li>
<li>Like Linear Regression with a categorical outcome</li>
<li>\(\hat{y} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Binary response variable (Y = 1 or Y = 0) association to a set of explanatory variables</li>
<li>Like Linear Regression with a categorical outcome</li>
<li>\(\hat{y} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\) becomes<br></li>
<li>\(\log{\frac{P(Y=1)}{1 - P(Y=1)}} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\)</li>
<li>Can be extended to multiple and/or ordered categories</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Family of GLMs
<ol>
<li>Random component <br>- Noise or Errors
<li>Systematic Component <br>- Linear combination in \(X_{i}\)
<li>Link Function <br>- Connects Random and Systematic components
</ol></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Data is I.I.D.

<ul>
<li>\(Y\)&#39;s assume to come from family of exponential distributions</li>
</ul></li>
<li>Uses MLE to determine parameters - Not OLS

<ul>
<li>MLE satisfies lots of nice properties (unbiased, consistent)</li>
<li>Does not require transformation of \(Y\)&#39;s to be Normal</li>
<li>Does not require constant variance</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Type of regression to predict the probability of being in a class

<ul>
<li>Output is \(P(Y=1\hspace{2 mm} |\hspace{2 mm} X)\)</li>
<li>Typical threshold is 0.5</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Type of regression to predict the probability of being in a class

<ul>
<li>Output is \(P(Y=1\hspace{2 mm} |\hspace{2 mm} X)\)</li>
<li>Typical threshold is 0.5</li>
</ul></li>
<li>Sigmoid (logistic) function: \(g(z) = \frac{1}{1+e^{-z}}\)

<ul>
<li>Bounded by 0 and 1</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="figure/log_curve.png" alt="plot of chunk log_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>The hypothesis function, \(h_{\theta}(x)\), is \(P(Y=1\hspace{2 mm} |\hspace{2 mm} X)\)</li>
<li>Linear regression: \(h_{\theta}(x) = \theta x^{T}\)<br>
(Recall that \(\theta x^{T} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\))</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>The hypothesis function, \(h_{\theta}(x)\), is \(P(Y=1\hspace{2 mm} |\hspace{2 mm} X)\)</li>
<li>Linear regression: \(h_{\theta}(x) = \theta x^{T}\)<br>
(Recall that \(\theta x^{T} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\))</li>
<li>Logistic regression: \(h_{\theta}(x) = g(\theta x^{T})\) 
<br>
where \(g(z) = \frac{1}{1+e^{-z}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>Re-arranging \(Y = \frac{1}{1+e^{-\theta x^{T}}}\) yields
<br>
\(\log{\frac{Y}{1 - Y}} = \theta x^{T}\), &quot;log odds&quot;</li>
<li>Log odds are linear in \(X\)</li>
<li>This is called the logit of \(Y\)

<ul>
<li>Links the odds of \(Y\) (a probability) to a linear regression in \(X\)</li>
<li>Logit ranges from -ve infite to +ve infinite</li>
<li>When \(x_{1}\) increases by 1 unit, \(P(Y=1\hspace{2 mm} |\hspace{2 mm} X)\) increases by \(e^{\theta_{1}}\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>Cost function?</li>
<li>Why can&#39;t we use the same cost function as for the linear hypothesis?</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>Cost function?</li>
<li>Why can&#39;t we use the same cost function as for the linear hypothesis?

<ul>
<li>Logistic residuals are Binomially distributed</li>
<li>Regression function is not linear in \(X\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y \hspace{2 mm} | \hspace{2 mm} X\) ~ Bernoulli</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y \hspace{2 mm} | \hspace{2 mm} X\) ~ Bernoulli</li>
<li>\(P(Y\hspace{2 mm} |\hspace{2 mm} X) = p\), when \(Y\) = 1 </li>
<li>\(P(Y\hspace{2 mm} |\hspace{2 mm} X) = 1-p\), when \(Y\) = 0</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y \hspace{2 mm} | \hspace{2 mm} X\) ~ Bernoulli</li>
<li>\(P(Y\hspace{2 mm} |\hspace{2 mm} X) = p\), when \(Y\) = 1 </li>
<li>\(P(Y\hspace{2 mm} |\hspace{2 mm} X) = 1-p\), when \(Y\) = 0</li>
<li>\(P(Y = y_{i}|X) = p^{y_{i}}(1-p)^{1-y_{i}}\)</li>
<li>Taking the log of both sides...</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(cost(p, y) = -y \log(p) + (1-y) \log(1-p)\)<br></li>
<li>\(p = h_{\theta}(x)\)</li>
<li>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
</ul>

<p><img src="figure/cost_curves1.png" alt="plot of chunk cost_curves"> <img src="figure/cost_curves2.png" alt="plot of chunk cost_curves"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x_{i})) + (1-y) \log(1-h_{\theta}(x_{i}))\)</li>
<li>Minimize the cost</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>Cannot solve analytically</li>
<li>Use approximation methods

<ul>
<li>(Stochastic) Gradient Descent</li>
<li>Conjugate Descent</li>
<li>Newton-Raphson Method</li>
<li>BFGS</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Class of hill-climbing techniques</li>
<li>Efficient</li>
<li>Easier to calculate than gradient descent

<ul>
<li>Except for first and second derivatives</li>
</ul></li>
<li>Fast</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
<li>Re-write \(f(x)\) as its Taylor expansion:<br>
\(f(x) = f(x_{0}) + (x-x_{0})f'(x_{0}) + \frac{1}{2}(x-x_{0})^{2}f''(x_{0})\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
<li>Re-write \(f(x)\) as its Taylor expansion:<br>
\(f(x) = f(x_{0}) + (x-x_{0})f'(x_{0}) + \frac{1}{2}(x-x_{0})^{2}f''(x_{0})\)</li>
<li>Take the derivative w.r.t \(x\) and set = 0<br>
\(0 = f'(x_{0}) + \frac{1}{2}f''(x_{0})2(x_{1} − x_{0})\)<br>
\(x_{1} = x_{0} − \frac{f'(x_{0})}{f￼''(x_{0})}\)

<ul>
<li>\(x_{1}\) is a better approximation for the minimum than \(x_{0}\)</li>
<li>and so on...</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<p>\(f(x) = x^{4} - 3\log(x)\)</p>

<p><img src="figure/newton_curve.png" alt="plot of chunk newton_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code class="r">fn &lt;- function(x) x^4 - 3*log(x)
dfn &lt;- function(x) 4*x^3 - 3/x
d2fn &lt;- function(x) 12*x^2 + 3/x^2 

newton &lt;- function(num.its, dfn, d2fn){
  theta &lt;- rep(0,num.its)
  theta[1] &lt;- round(runif(1,0,100),0)

  for (i in 2:num.its) {
    h &lt;- - dfn(theta[i-1]) / d2fn(theta[i-1])
    theta[i] &lt;- theta[i-1] + h 
  }

  out &lt;- cbind(1:num.its,theta)
  dimnames(out)[[2]] &lt;- c(&quot;iteration&quot;,&quot;estimate&quot;)
  return(out)
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code>     iteration estimate
[1,]         1    54.00
[2,]         2    36.00
[3,]         3    24.00
[4,]         4    16.00
[5,]         5    10.67
</code></pre>

<pre><code>      iteration estimate
[16,]        16   0.9306
[17,]        17   0.9306
[18,]        18   0.9306
[19,]        19   0.9306
[20,]        20   0.9306
</code></pre>

<pre><code>0.9658     ## value of f(x) at minimum
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code class="r">optimize(fn,c(-100,100))  ## built-in R optimization function
</code></pre>

<pre><code>$minimum
[1] 0.9306

$objective
[1] 0.9658
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Minimization algorithm</li>
<li>Approximation, non-closed form solution</li>
<li>Built-in to many programs</li>
<li>Can be used to find the parameters of a logistic regression equation</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Very popular classification algorithm</li>
<li>Part of family of GLMs</li>
<li>Based on Binomial error terms, 1&#39;s and 0&#39;s</li>
<li>Usually requires large sample size</li>
<li>Assumes linearity between logit function and independent variables</li>
<li>Uses sigmoid to link the probabilities with regression</li>
<li>Does not work out of the box with correlated features...</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Objectives</h1>

<p><space></p>

<ol>
<li>Motivation and examples</li>
<li>Eigenvalues</li>
<li>Derivation</li>
<li>Example</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Unsupervised learning</li>
<li>Used widely in modern data analysis</li>
<li>Compute the most meaningful way to re-express noisy data, revealing the hidden structure</li>
<li>Commonly used to supplement supervised learning algorithms</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Rpres_ML_2/figure/original_data.png" alt="original_data"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="..Rpres_ML_2/figure/calc_centroid.png" alt="calc_centroid"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/sub_mean.png" alt="sub_mean"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/max_var_dir.png" alt="max_var_dir"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/second_PC.png" alt="second_PC"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/rotated_grid.png" alt="rotated_grid"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/rotated_PCs.png" alt="rotated_PCs"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/new_axes.png" alt="new_axes"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/final_PC.png" alt="final_PC"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Assumes linearity</li>
<li>\(\bf{PX}=\bf{Y}\)

<ul>
<li>\(\bf{X}\) is original dataset, \(\bf{P}\) is a transformation of \(\bf{X}\) into \(\bf{Y}\)</li>
</ul></li>
<li>How to choose \(\bf{P}\)?<br>

<ul>
<li>Reduce noise (redundancy)<br></li>
<li>Maximize signal (variance)</li>
<li>Provides most information</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Covariance matrix is square, symmetric</li>
<li>\(\bf{C_{x}} = \frac{1}{(n-1)}\bf{XX^{T}}\)</li>
<li>Diagonals are variances, off-diagonals are covariances

<ul>
<li>Goal: maximize diagonals and minimize off-diagonals</li>
</ul></li>
<li>The optimal \(\bf{Y}\) would have a covariance matrix, \(\bf{C_{Y}}\), with positive values on the diagonal and 0&#39;s on the off-diagonals

<ul>
<li>Diagonalization</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>The Objective</h1>

<p><space></p>

<ul>
<li>Find some matrix \(\bf{P}\) where \(\bf{PX}=\bf{Y}\) such that \(\bf{Y}\)&#39;s covariance matrix is diagonalized

<ul>
<li>The rows of \(\bf{P}\) are the Principal components</li>
<li>PCA by &quot;eigen decomposition&quot;</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>Eigenvalues help uncover valuable insight into the underlying structure of a vector space</li>
<li>Eigenvalues/vectors come up extensively in physics, engineering, statistics<br><br></li>
<li>Eigenvalues are scalars derived from a square matrix, &quot;characteristic roots&quot;</li>
<li>Eigenvectors are non-zero vectors associated with eigenvalues</li>
<li>Almost every square matrix has at least 1 eigenvalue/vector combo (otherwise its &quot;degenerative&quot;)</li>
<li>Decomposing a square matrix into eigenvalues/vectors is eigen decomposition</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(\bf{A}x = \lambda x\)</p>

<ul>
<li>\(\lambda\) is an eigenvalue of \(\bf{A}\) and \(\bf{x}\) is an eigenvector of \(\bf{A}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(\bf{A}x = \lambda x\)<br>
\(\bf{A}x - \lambda Ix = 0\)<br>
\((\bf{A} - \lambda I)x = 0\)<br>
For this to be non-trivial \(\det(\bf{A} - \lambda I)\) = 0<br></p>

<ul>
<li>roots are eigenvalues of \(\bf{A}\)</li>
<li>characteristic polynomial of \(\bf{A}\)</li>
<li>\({\lambda}\) is called the spectrum</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X = \lambda X\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X = \lambda X\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X - \lambda X = 0\]
\[(\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda I)X = 0\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<p>\[\left | \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda I \right |= 0\]
\[\left|\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} \right| = 0\]
\[\left|\begin{bmatrix} 5-\lambda & 2\\ 2 & 5-\lambda \end{bmatrix}\right| = 0\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<p>\((5-\lambda)\times(5-\lambda) - 4 = 0\)
<br>
\(\lambda^{2} - 10\lambda + 21 = 0\)
<br>
\(\lambda = ?\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenExample</h1>

<p><space></p>

<pre><code class="r">A = matrix(c(5,2,2,5),nrow=2)
roots &lt;- Re(polyroot(c(21,-10,1)))
roots
</code></pre>

<pre><code>## [1] 3 7
</code></pre>

<p>\(\lambda = 3, 7\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(\bf{Ax} = 3\bf{x}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(\bf{Ax} = 3\bf{x}\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(\bf{Ax} = 3\bf{x}\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br>
\(x_{1} = -x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(\bf{Ax} = 3\bf{x}\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br>
\(x_{1} = -x_{2}\)<br></li>
</ul>

<p>\[Eigenvector = \begin{bmatrix} 1\\ -1 \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 7\)<br>
\(\bf{Ax} = 7\bf{x}\)<br>
\(5x_{1} + 2x_{2} = 7x_{1}\)<br>
\(2x_{2} + 5x_{2} = 7x_{2}\)<br>
\(x_{1} = x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 7\)<br>
\(\bf{Ax} = 7\bf{x}\)<br>
\(5x_{1} + 2x_{2} = 7x_{1}\)<br>
\(2x_{2} + 5x_{2} = 7x_{2}\)<br>
\(x_{1} = x_{2}\)<br></li>
</ul>

<p>\[Eigenvector = \begin{bmatrix} 1\\ 1 \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<p>\(\bf{Ax} = \bf{\lambda x}\)</p>

<pre><code class="r">x1 = c(1,-1)
x2 = c(1,1)
A %*% x1 == 3 * x1
A %*% x2 == 7 * x2
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigencheck</h1>

<p><space></p>

<p>\(\bf{Ax} = \lambda \bf{x}\)</p>

<pre><code class="r">A %*% x1 == 3 * x1
</code></pre>

<pre><code>     [,1]
[1,] TRUE
[2,] TRUE
</code></pre>

<pre><code class="r">A %*% x2 == 7 * x2
</code></pre>

<pre><code>     [,1]
[1,] TRUE
[2,] TRUE
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Diagonalization</h1>

<p><space></p>

<ul>
<li>If \(\bf{A}\) has n linearly independent eigenvectors, then it is diagonalizable

<ul>
<li>Written in the form \(\bf{A} = \bf{PD{P}^{-1}}\)</li>
<li>\(\bf{P}\) is row matrix of eigenvectors</li>
<li>\(\bf{D}\) is diagonal matrix of eigenvalues of \(\bf{A}\)</li>
<li>\(\bf{A}\) is similar to \(\bf{D}\)</li>
</ul></li>
<li>Eigenvalues of a symmetric matrix can form a new basis (this is what we want!)</li>
<li>If the eigenvectors are orthonormal, then \(\bf{{P}^{T} = {P}^{-1}}\)<br>
\(\bf{A} = \bf{PD{P}^{T}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Diagonalization</h1>

<p><space></p>

<p>\(\bf{A} = \bf{PDP^{T}}\)</p>

<pre><code class="r">m &lt;- matrix(c(x1,x2),ncol=2)
m &lt;- m/sqrt(norm(m))  ## normalize
as.matrix(m %*% diag(roots) %*% t(m))
</code></pre>

<pre><code>##      [,1] [,2]
## [1,]    5    2
## [2,]    2    5
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>EigenDecomposition summary</h1>

<p><space></p>

<ul>
<li>Eigenvalue and eigenvectors are important</li>
<li>Linear Algebra theorems allow for matrix manipulation</li>
<li>Steps to eigendecomposition:

<ul>
<li>1) Set up characteristic equation</li>
<li>2) Solve for eigenvalues by finding roots of equation</li>
<li>3) Plug eigenvalues back in to find eigenvectors</li>
</ul></li>
<li>There&#39;s a lot more to eigenvalues!</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Objective</h1>

<p><space></p>

<ul>
<li>Find some matrix \(\bf{P}\) where \(\bf{PX}=\bf{Y}\) such that \(\bf{Y}\)&#39;s covariance matrix is diagonalized</li>
<li>Covariance matrix<br>
\(\bf{C_{X}} = \frac{1}{(n-1)}\bf{XX}^{T}\)

<ul>
<li>Diagonals are the variances, off-diagonals are covariances</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Proof</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Proof</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br>
\(=\frac{1}{(n-1)}\bf{PX(PX)^{T}}\)<br>
\(=\frac{1}{(n-1)}\bf{P(XX^{T})P^{T}}\),  because \((AB)^{T} = B^{T}A^{T}\)<br></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Proof</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br>
\(=\frac{1}{(n-1)}\bf{PX(PX)^{T}}\)<br>
\(=\frac{1}{(n-1)}\bf{P(XX^{T})P^{T}}\),  because \((AB)^{T} = B^{T}A^{T}\)<br> 
\(=\frac{1}{(n-1)}\bf{PAP^{T}}\)<br></p>

<ul>
<li>\(\bf{P}\) is a matrix with rows that are eigenvectors</li>
<li>\(\bf{A}\) is a diagonalized matrix of eigenvalues and is symmetric<br>
\(\bf{A} = \bf{EDE^{T}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Each row of \(\bf{P}\) should be an eigenvector of \(\bf{A}\)</li>
<li>Therefore we are forcing this relationship to hold \(\bf{P} = \bf{E^{T}}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Each row of \(\bf{P}\) should be an eigenvector of \(\bf{A}\)<br></li>
<li>Therefore we are forcing this relationship to hold \(\bf{P} = \bf{E^{T}}\)<br>
Since \(\bf{A} = \bf{EDE^{T}}\)<br>
\(\bf{A} = \bf{P^{T}DP}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Each row of \(\bf{P}\) should be an eigenvector of \(\bf{A}\)<br></li>
<li>Therefore we are forcing this relationship to hold \(\bf{P} = \bf{E^{T}}\)<br>
Since \(\bf{A} = \bf{EDE^{T}}\)<br>
\(\bf{A} = \bf{P^{T}DP}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{PAP^{T}}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{P(P^{T}DP)P^{T}}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{(PP^{-1})D(PP^{-1})}\), because \(\bf{P^{T}}=\bf{P^{-1}}\)<br>
\(= \frac{1}{n-1}\bf{D}\)</li>
<li>Therefore \(\bf{C_{Y}}\) is diagonalized</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Assumptions</h1>

<p><space></p>

<ul>
<li>Assumes linear relationship between \(\bf{X}\) and \(\bf{Y}\) (non-linear is a kernel PCA)</li>
<li>Orthogonal components - ensures no correlation among PCs</li>
<li>Largest variance indicates most signal</li>
<li>Assumes data is normally distributed, otherwise PCA might not diagonalize matrix

<ul>
<li>Can use ICA...</li>
<li>But most data is normal and PCA is robust to slight deviance from normality</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">data &lt;- read.csv(&#39;tennis_data_2013.csv&#39;)
data$Player1 &lt;- as.character(data$Player1)
data$Player2 &lt;- as.character(data$Player2)

tennis &lt;- data
m &lt;- length(data)

for (i in 10:m){
  tennis[,i] &lt;- ifelse(is.na(data[,i]),0,data[,i])
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">features &lt;- tennis[,10:m]

head(features)
</code></pre>

<pre><code>##   FSP.1 FSW.1 SSP.1 SSW.1 ACE.1 DBF.1 WNR.1 UFE.1 BPC.1 BPW.1 NPA.1 NPW.1
## 1    61    35    39    18     5     1    17    29     1     3     8    11
## 2    61    31    39    13    13     1    13     1     7    14     0     0
## 3    52    53    48    20     8     4    37    50     1     9    16    23
## 4    53    39    47    24     8     6     8     6     6     9     0     0
## 5    76    63    24    12     0     4    16    35     3    12     9    13
## 6    65    51    35    22     9     3    35    41     2     7     6    12
##   TPW.1 FSP.2 FSW.2 SSP.2 SSW.2 ACE.2 DBF.2 WNR.2 UFE.2 BPC.2 BPW.2 NPA.2
## 1    70    68    45    32    17    10     0    40    30     4     8     8
## 2    80    60    23    40     9     1     4     1     4     0     0     0
## 3   106    77    57    23    15     9     1    41    41     4    13    12
## 4   104    50    24    50    19     1     8     1     8     1     7     0
## 5   128    53    59    47    32    17    11    59    79     3     5    16
## 6   108    63    60    37    22    24     4    47    45     4     7    14
##   NPW.2 TPW.2
## 1     9   101
## 2     0    42
## 3    16   126
## 4     0    79
## 5    28   127
## 6    17   122
</code></pre>

<pre><code class="r">str(features)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    943 obs. of  26 variables:
##  $ FSP.1: int  61 61 52 53 76 65 68 47 64 77 ...
##  $ FSW.1: int  35 31 53 39 63 51 73 18 26 76 ...
##  $ SSP.1: int  39 39 48 47 24 35 32 53 36 23 ...
##  $ SSW.1: int  18 13 20 24 12 22 24 15 12 11 ...
##  $ ACE.1: num  5 13 8 8 0 9 5 3 3 6 ...
##  $ DBF.1: num  1 1 4 6 4 3 3 4 0 4 ...
##  $ WNR.1: num  17 13 37 8 16 35 41 21 20 6 ...
##  $ UFE.1: num  29 1 50 6 35 41 50 31 39 4 ...
##  $ BPC.1: num  1 7 1 6 3 2 9 6 3 7 ...
##  $ BPW.1: num  3 14 9 9 12 7 17 20 7 24 ...
##  $ NPA.1: num  8 0 16 0 9 6 14 6 5 0 ...
##  $ NPW.1: num  11 0 23 0 13 12 30 9 14 0 ...
##  $ TPW.1: num  70 80 106 104 128 108 173 78 67 162 ...
##  $ FSP.2: int  68 60 77 50 53 63 60 54 67 60 ...
##  $ FSW.2: int  45 23 57 24 59 60 66 26 42 68 ...
##  $ SSP.2: int  32 40 23 50 47 37 40 46 33 40 ...
##  $ SSW.2: int  17 9 15 19 32 22 34 13 14 25 ...
##  $ ACE.2: num  10 1 9 1 17 24 2 0 12 8 ...
##  $ DBF.2: num  0 4 1 8 11 4 6 11 0 12 ...
##  $ WNR.2: num  40 1 41 1 59 47 57 11 32 8 ...
##  $ UFE.2: num  30 4 41 8 79 45 72 46 20 12 ...
##  $ BPC.2: num  4 0 4 1 3 4 10 2 7 6 ...
##  $ BPW.2: num  8 0 13 7 5 7 17 6 10 14 ...
##  $ NPA.2: num  8 0 12 0 16 14 25 8 8 0 ...
##  $ NPW.2: num  9 0 16 0 28 17 36 12 11 0 ...
##  $ TPW.2: num  101 42 126 79 127 122 173 61 94 141 ...
</code></pre>

<pre><code class="r">dim(features)
</code></pre>

<pre><code>## [1] 943  26
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">## Manually Calculated PCs
scaled_features &lt;- as.matrix(scale(features))
Cx &lt;- cov(scaled_features)
eigenvalues &lt;- eigen(Cx)$values
eigenvectors &lt;- eigen(Cx)$vectors
PC &lt;- scaled_features %*% eigenvectors
Cy &lt;- cov(PC)
</code></pre>

<ul>
<li>Cy should be diagonalized matrix

<ul>
<li>diagonals of Cy should be the eigenvalues of Cx</li>
<li>off diagonals should be 0</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">sum_diff &lt;- (sum(diag(Cy) - eigenvalues))^2
round(sum_diff,6)
</code></pre>

<pre><code>## [1] 0
</code></pre>

<pre><code class="r">off_diag &lt;- upper.tri(Cy)|lower.tri(Cy) ## remove diagonal elements
round(sum(Cy[off_diag]),6)   ## off diagonals are 0 since PC&#39;s are orthogonal
</code></pre>

<pre><code>## [1] 0
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p><img src="figure/var_expl_plot.png" alt="plot of chunk var_expl_plot"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">pca.df &lt;- prcomp(scaled_features)  ## Built in R function
round(eigenvalues,10) == round((pca.df$sdev)^2,10)  ## Manual calculation matches R
</code></pre>

<pre><code> [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
[15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
</code></pre>

<pre><code class="r">round(eigenvectors[,1],10) == round(pca.df$rotation[,1],10)
</code></pre>

<pre><code>FSP.1 FSW.1 SSP.1 SSW.1 ACE.1 DBF.1 WNR.1 UFE.1 BPC.1 BPW.1 NPA.1 NPW.1 
 TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE 
TPW.1 FSP.2 FSW.2 SSP.2 SSW.2 ACE.2 DBF.2 WNR.2 UFE.2 BPC.2 BPW.2 NPA.2 
 TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE 
NPW.2 TPW.2 
 TRUE  TRUE 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>Can the first two Principal Components separate our data?</li>
</ul>

<p><img src="figure/tennis_plot_gender.png" alt="plot of chunk tennis_plot_gender"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>Classify based on PC1?</li>
</ul>

<pre><code class="r">PC1 &lt;- pca.df$x[,1]
mean_PC1 &lt;- mean(pca.df$x[,1])
gen &lt;- ifelse(PC1 &gt; mean_PC1,&quot;F&quot;,&quot;M&quot;)
sum(diag(table(gen,as.character(data$Gender))))/rows
</code></pre>

<pre><code>[1] 0.7646
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>Principal Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Very popular dimensionality reduction technique</li>
<li>Intuitive</li>
<li>Cannot reverse engineer dataset easily</li>
<li>Sparse PCA emphasizes important features</li>
<li>Non-linear structure is difficult to model with PCA</li>
<li>Extensions (ICA, kernel PCA) developed to generalize</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Objectives</h1>

<p><space></p>

<ol>
<li>Motivation and examples</li>
<li>Kmeans</li>
<li>DBSCAN</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Separate data into meaningful or useful groups

<ul>
<li>Capture natural structure of the data</li>
<li>Starting point for further analysis</li>
</ul></li>
<li>Cluster for utility

<ul>
<li>Summarizing data for less expensive computation</li>
<li>Data compression</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Types of Clusters</h1>

<p><space></p>

<ul>
<li>Data that looks similar</li>
<li>Prototype based</li>
<li>Density based</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Typical clustering problem</h1>

<p><space></p>

<p><img src="figure/cluster_plot_example.png" alt="plot of chunk cluster_plot_example"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-92" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Density based cluster</h1>

<p><space></p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/0/05/DBSCAN-density-data.svg" density_based /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-93" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>One of the simplest unsupervised learning algorithms</li>
<li>Group points into clusters; clusters center around a centroid</li>
<li>Minimize the distance between a points and its centroid</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-94" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ul>
<li>Select K points as initial centroids </li>
<li>Do

<ul>
<li>Form K clusters by assigning each point to its closest centroid</li>
<li>Recompute the centroid of each cluster </li>
</ul></li>
<li>Until centroids do not change, or change very minimally, i.e. &lt; 1%</li>
<li>Computatinal complexity: \(O(nkI)\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-95" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ul>
<li>Use similarity measures (Euclidean or cosine) depending on the data</li>
<li>Minimize the squared distance of each point to closest centroid
\(SSE(k) = \sum_{i=1}^{m}\sum_{j=1}^{n} (x_{ij} - \bar{x}_{kj})^2\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-96" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans - notes</h1>

<p><space></p>

<ul>
<li>There is no &quot;correct&quot; number of clusters</li>
<li>Choose initial K randomly 

<ul>
<li>Can lead to poor centroids - local minimum</li>
<li>Run kmeans multiple times</li>
</ul></li>
<li>Reduce the total SSE by increasing K</li>
<li>Increase the cluster with largest SSE</li>
<li>Split up a cluster into other clusters

<ul>
<li>The centroid that is split will increase total SSE the least</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-97" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans - notes</h1>

<p><space></p>

<ul>
<li>Bisecting Kmeans

<ul>
<li>Split points into 2 clusters</li>
<li>Take cluster with largest SSE - split that into two clusters</li>
<li>Rerun bisecting Kmeans on resulting clusters</li>
<li>Stop when you have K clusters</li>
</ul></li>
<li>Less susceptible to initialization problems</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-98" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/different_density.png" alt="different_density"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-99" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/different_size_clusters.png" alt="different_size_clusters"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-100" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/non-globular.png" alt="non-globular"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-101" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans - example</h1>

<p><space></p>

<pre><code class="r">wine &lt;- read.csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#39;)
names(wine) &lt;- c(&quot;class&quot;,&#39;Alcohol&#39;,&#39;Malic&#39;,&#39;Ash&#39;,&#39;Alcalinity&#39;,&#39;Magnesium&#39;,&#39;Total_phenols&#39;,
                 &#39;Flavanoids&#39;,&#39;NFphenols&#39;,&#39;Proanthocyanins&#39;,&#39;Color&#39;,&#39;Hue&#39;,&#39;Diluted&#39;,&#39;Proline&#39;)
str(wine[,1:7])
</code></pre>

<pre><code>## &#39;data.frame&#39;:    177 obs. of  7 variables:
##  $ class        : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Alcohol      : num  13.2 13.2 14.4 13.2 14.2 ...
##  $ Malic        : num  1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 2.16 ...
##  $ Ash          : num  2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 2.3 ...
##  $ Alcalinity   : num  11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 18 ...
##  $ Magnesium    : int  100 101 113 118 112 96 121 97 98 105 ...
##  $ Total_phenols: num  2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 2.95 ...
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-102" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans - example</h1>

<p><space></p>

<p><img src="figure/unnamed-chunk-1.png" alt="plot of chunk unnamed-chunk-1"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-103" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<ul>
<li>A cluster is a dense region of points separated by low-density regions</li>
<li>Group objects into one cluster if they are connected to one another by densely populated area</li>
<li>Used when the clusters are irregularly shaped, and when noise and outliers are present</li>
<li>computational complexity: \(O(n\log{n})\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-104" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, \(Eps\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-105" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, \(Eps\)<br>
<img src="Rpres_ML_2/figure/density_structure.png" alt="density"></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-106" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, \(Eps\)</li>
<li>High density region has at least \(Minpts\) within \(Eps\) of point p</li>
<li>Noise points are not within \(Eps\) of border or core points</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-107" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, \(Eps\)</li>
<li>High density region has at least \(Minpts\) within \(Eps\) of point p</li>
<li>Noise points are not within \(Eps\) of border or core points</li>
<li>If p is density connected to q, they are part of the same cluster, if not, then they are not</li>
<li>If p is not density connected to any other point, considered noise</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-108" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/density_ex_win.png" alt="density_win"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-109" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<p><img src="figure/unnamed-chunk-2.png" alt="plot of chunk unnamed-chunk-2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-110" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<p><img src="figure/dbscan_ex.png" alt="plot of chunk dbscan_ex"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-111" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Unsupervised learning</li>
<li>Not a perfect science - lots of interpretation

<ul>
<li>Dependent on values of K, \(Eps\), \(Minpts\)</li>
</ul></li>
<li>Hard to define &quot;correct&quot; clustering</li>
<li>Many types of algorithms</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-112" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Objectives</h1>

<p><space></p>

<ol>
<li>Structure</li>
<li>Entropy</li>
<li>Boosting and error cost</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-113" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/tree_example.png" alt="overview"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-114" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/tree_structure.png" alt="structure"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-115" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<ul>
<li>Recursive partitioning -&gt; &quot;divide and conquer&quot;</li>
<li>Going down, choose feature that is most <em>predictive</em> of target class

<ul>
<li>Split the data according to feature</li>
<li>Continue...</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-116" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p>Until...</p>

<ul>
<li>All examples at a node are in same class</li>
<li>No more features left to distinguish (as a result, prone to overfitting)</li>
<li>Tree has grown to some prespecified limit (called pruning)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-117" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Algorithms</h1>

<p><space></p>

<ul>
<li>ID3

<ul>
<li>Original, popular, DT implementation</li>
</ul></li>
<li>C4.5: Like ID3 +

<ul>
<li>Handles continuous cases</li>
<li>Imputing missing values</li>
<li>Weighing costs</li>
<li>Pruning post creation</li>
</ul></li>
<li>C5.0: Like C4.5 + 

<ul>
<li>Faster, less memory usage</li>
<li>Boosting</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-118" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Selecting features</h1>

<p><space></p>

<ul>
<li>How to select feature?

<ul>
<li>Purity of resulting split</li>
<li>After splitting, how homogenous are the resulting classes?</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-119" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Selecting features</h1>

<p><space></p>

<ul>
<li>How to select feature?

<ul>
<li>Purity of resulting split</li>
<li>After splitting, how homogenous are the resulting classes?</li>
</ul></li>
<li>Entropy: amount of information contained in a random variable

<ul>
<li>For a feature with N classes:<br>
&nbsp;&nbsp;- 0 = purely homogenous<br>
&nbsp;&nbsp;- \(\log_{2}(N)\) = completely mixed</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-120" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<p>\(Entropy(S) = \sum_{i=1}^{c} -p_{i}\log_{2}(p_{i})\)</p>

<ul>
<li>where \(S\) is a dataset</li>
<li>\(c\) is the number of classes in that data</li>
<li>\(p_{i}\) is the proportion of values in that class</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-121" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a fair, 6 sided die?</p>

<pre><code class="r">entropy &lt;- function(probs){
  # probs is a list of probabilities
  ent &lt;- 0
  for(i in probs){
    ent_temp &lt;- -i*log2(i)
    ent &lt;- ent + ent_temp
  }
  return(ent)
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-122" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">fair &lt;- rep(1/6,6)
entropy(fair)
</code></pre>

<pre><code>FALSE [1] 2.585
</code></pre>

<pre><code class="r">log2(6)
</code></pre>

<pre><code>FALSE [1] 2.585
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-123" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a biased, 6 sided die?<br>
\(P(X=1) = P(X=2) = P(X=3) = 1/9\)<br>
\(P(X=4) = P(X=5) = P(X=6) = 2/9\)</p>

<pre><code class="r">biased &lt;- c(rep(1/9,3),rep(2/9,3))
entropy(biased)
</code></pre>

<pre><code>[1] 2.503
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-124" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">more_biased &lt;- c(rep(1/18,3),rep(5/18,3))
entropy(more_biased)
</code></pre>

<pre><code>[1] 2.235
</code></pre>

<pre><code class="r">most_biased &lt;- c(rep(1/100,5),rep(95/100,1))
entropy(most_biased)
</code></pre>

<pre><code>[1] 0.4025
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-125" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">curve(-x*log2(x)-(1 - x)*log2(1 - x), col =&quot; red&quot;, xlab = &quot;x&quot;, ylab = &quot;Entropy&quot;, 
      lwd = 4, main=&#39;Entropy of a coin toss&#39;)
</code></pre>

<p><img src="figure/entropy_curve.png" alt="plot of chunk entropy_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-126" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<ul>
<li>C5.0 uses the change in entropy to determine the change in purity</li>
<li>InfoGain = Entropy (pre split) - Entropy (post split)

<ul>
<li>Entropy (pre split) &gt;&gt; current Entropy</li>
<li>Entropy (post split) &gt;&gt; need to consider Entropy of each possible split</li>
<li>\(Entropy(post) = \sum_{i=1}^{n}w_{i}Entropy(P_{i})\)
<br></li>
</ul></li>
<li>Notes:

<ul>
<li>The more a feature splits the data in obvious ways the less informative it is</li>
<li>The more a feature splits the data - in general - the more information is gained by splitting at that feature</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-127" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">voting_data &lt;- read.csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&#39;)
names(voting_data) &lt;- c(&#39;party&#39;,&#39;handicapped-infants&#39;,&#39;water-project-cost-sharing&#39;,
                        &#39;adoption-of-the-budget-resolution&#39;,&#39;physician-fee-freeze&#39;,
                        &#39;el-salvador-aid&#39;,&#39;religious-groups-in-schools&#39;,
                        &#39;anti-satellite-test-ban&#39;,&#39;aid-to-nicaraguan-contras&#39;,
                        &#39;mx-missile&#39;,&#39;immigration&#39;,&#39;synfuels-corporation-cutback&#39;,
                        &#39;education-spending&#39;,&#39;superfund-right-to-sue&#39;,&#39;crime&#39;,
                        &#39;duty-free-exports&#39;,&#39;export-administration-act-south-africa&#39;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-128" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">prop.table(table(voting_data[,1]))
</code></pre>

<pre><code>
  democrat republican 
    0.6152     0.3848 
</code></pre>

<pre><code class="r">n &lt;- nrow(voting_data)
train_ind &lt;- sample(n,2/3*n)
voting_train &lt;- voting_data[train_ind,]
voting_test &lt;- voting_data[-train_ind,]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-129" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p><img src="Rpres_ML_2/figure/real_tree_example.png" height="500px" width="500px" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-130" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code>            tree_predict
             democrat republican
  democrat         78          7
  republican        1         59
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-131" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">head(C5imp(tree_model))   # most important variables
</code></pre>

<pre><code>                                  Overall
physician-fee-freeze                97.58
handicapped-infants                  0.00
water-project-cost-sharing           0.00
adoption-of-the-budget-resolution    0.00
el-salvador-aid                      0.00
religious-groups-in-schools          0.00
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-132" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Boosting</h1>

<p><space></p>

<ul>
<li>Combine a bunch of weak learners to create a team that is much stronger</li>
<li>This is where C5.0 improves on C4.5</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-133" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Boosting Example</h1>

<p><space></p>

<pre><code class="r">boosted_tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=25)
boosted_tennis_predict &lt;- predict(boosted_tree_model,voting_test[,-1])
boosted_conf &lt;- table(voting_test[,1], boosted_tennis_predict)
boosted_conf
</code></pre>

<pre><code>            boosted_tennis_predict
             democrat republican
  democrat         78          7
  republican        1         59
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-134" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<ul>
<li>Still getting too many false positives (predict Republican but actually Democrat)</li>
<li>Introduce higher cost to getting this wrong</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-135" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<pre><code class="r">error_cost &lt;- matrix(c(0,1,2,0),nrow=2)
cost_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1, costs = error_cost)
cost_predict &lt;- predict(cost_model, newdata=voting_test[,-1])
conf &lt;- table(voting_test[,1], cost_predict)
conf
</code></pre>

<pre><code>            cost_predict
             democrat republican
  democrat         75         10
  republican        0         60
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-136" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<p><img src="figure/plot_boost_acc.png" alt="plot of chunk plot_boost_acc"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-137" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Pros and Cons</h1>

<p><space></p>

<ul>
<li>Trees are non-parametric, rule based classification or regression method</li>
<li>Simple to understand and interpret</li>
<li>Little data preparation</li>
<li>Works well with small or large number of features
<br></li>
<li>Easy to overfit</li>
<li>Biased towards splits on features with large number of levels</li>
<li>Usually finds local optimum</li>
<li>Difficult concepts are hard to learn</li>
<li>Avoid pre-pruning</li>
<li>Hard to know optimal length of tree without growing it there first</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-138" style="background:;">
  <hgroup>
    <h2>Summary</h2>
  </hgroup>
  <article>
    <h1>ML - Part II</h1>

<p><space></p>

<ul>
<li>Logistic regression</li>
<li>Math behind PCA</li>
<li>Clustering basics</li>
<li>Trees and improvements</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-139" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article>
    <p><space></p>

<ul>
<li><a href="http://www.packtpub.com/machine-learning-with-r/book">Machine Learning with R</a></li>
<li><a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a></li>
<li><a href="http://web.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-140" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>
