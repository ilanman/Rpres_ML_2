<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning with R - II</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning with R - II">
  <meta name="author" content="Ilan Man">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Machine Learning with R - II</h1>
        <h2></h2>
        <p>Ilan Man<br/>Strategy Operations  @ Squarespace</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Logistic Regression</li>
<li>Principle Component Analysis</li>
<li>Clustering</li>
<li>Trees</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Objectives</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Understand some popular algorithms and techniques</li>
<li>Learn how to tune parameters</li>
<li>Practice R</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<pre><code class="r">x &lt;- 1:10
log_ex &lt;- data.frame(Y = c(rnorm(5, 0, 0.01), rnorm(5, 5, 0.01)), X = x)
ggplot(log_ex, aes(X, Y)) + geom_point(color = &quot;blue&quot;, size = 3) + stat_smooth(method = &quot;lm&quot;, 
    se = F, color = &quot;green&quot;, size = 1)
</code></pre>

<p><img src="figure/log_bad_fit.png" alt="plot of chunk log_bad_fit"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<pre><code class="r">library(&quot;MASS&quot;)
library(ggplot2)
data(menarche)
log_data &lt;- data.frame(Y = menarche$Menarche/menarche$Total)
log_data$X &lt;- menarche$Age

glm.out &lt;- glm(cbind(Menarche, Total - Menarche) ~ Age, family = binomial(logit), 
    data = menarche)
lm.out &lt;- lm(Y ~ X, data = log_data)

log_data$fitted &lt;- glm.out$fitted

data_points &lt;- ggplot(log_data) + geom_point(aes(x = X, y = Y), color = &quot;blue&quot;, 
    size = 3)
line_points &lt;- data_points + geom_abline(intercept = coef(lm.out)[1], slope = coef(lm.out)[2], 
    color = &quot;green&quot;, size = 1)
curve_points &lt;- line_points + geom_line(aes(x = X, y = fitted), color = &quot;red&quot;, 
    size = 1)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>type of regression to predict the probability of being in a class

<ul>
<li>typical to set threshold to 0.5</li>
</ul></li>
<li>assumes error terms are Binomially distributed

<ul>
<li>which generates 1&#39;s and 0&#39;s as the error term</li>
</ul></li>
<li>sigmoid or logistic function: \(g(z) = \frac{1}{1+e^{-z}}\)

<ul>
<li>interpret the output as \(P(Y=1 | X)\)</li>
<li>bounded by 0 and 1</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<pre><code class="r">curve(1/(1 + exp(-x)), from = -10, to = 10, ylab = &quot;P(Y=1|X)&quot;, col = &quot;red&quot;, 
    lwd = 3)
abline(a = 0.5, b = 0, lty = 2, col = &quot;blue&quot;, lwd = 3)
</code></pre>

<p><img src="figure/log_curve.png" alt="plot of chunk log_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>The hypothesis function, \(h_{\theta}(x)\), is P(Y=1|X)</li>
<li>Linear Regression --&gt; \(h_{\theta}(x) = \theta x^{T}\)</li>
<li>Logistic Regression --&gt; \(h_{\theta}(x) = g(\theta x^{T})\) 
<br>
where \(g(z) = \frac{1}{1+e^{-z}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>Re-arranging \(Y = \frac{1}{1+e^{-\theta x^{T}}}\) yields \(\log{\frac{Y}{1 - Y}} = \theta x^{T}\)<br></li>
<li>&quot;log odds&quot;&quot; are linear in X</li>
<li>this is called the logit of theta

<ul>
<li>links X linearly with some function of Y</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>What is the cost function?</li>
<li>Why can&#39;t we use the same cost function as for the linear hypothesis?

<ul>
<li>logistic residuals are Binomially distributed - not Normal</li>
<li>the regression function is not linear in X</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>Define logistic cost function as:</li>
</ul>

<p>\(cost(h_{\theta}(x)):\)<br>
&nbsp;&nbsp; \(= -\log(x),\) &nbsp;&nbsp;&nbsp;  \(y = 1\)<br>
&nbsp;&nbsp; \(= -\log(1-x),\) &nbsp;   \(y = 0\)</p>

<p><img src="figure/cost_curves1.png" alt="plot of chunk cost_curves"> <img src="figure/cost_curves2.png" alt="plot of chunk cost_curves"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>using statistics, it can be shown that<br>
\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>using statistics, it can be shown that<br>
\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>using statistics, it can be shown that<br>
\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)</li>
<li>Minimize the cost</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<p><img src="figure/grad_ex_plot.png" alt="plot of chunk grad_ex_plot"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">x &lt;- cbind(1, x)  #Add ones to x  
theta &lt;- c(0, 0)  # initalize theta vector 
m &lt;- nrow(x)  # Number of the observations 
grad_cost &lt;- function(X, y, theta) return(sum(((X %*% theta) - y)^2))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">gradDescent &lt;- function(X, y, theta, iterations, alpha) {
    m &lt;- length(y)
    grad &lt;- rep(0, length(theta))
    cost.df &lt;- data.frame(cost = 0, theta = 0)

    for (i in 1:iterations) {
        h &lt;- X %*% theta
        grad &lt;- (t(X) %*% (h - y))/m
        theta &lt;- theta - alpha * grad
        cost.df &lt;- rbind(cost.df, c(grad_cost(X, y, theta), theta))
    }

    return(list(theta, cost.df))
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">## initialize X, y and theta
X1 &lt;- matrix(ncol = 1, nrow = nrow(df), cbind(1, df$X))
Y1 &lt;- matrix(ncol = 1, nrow = nrow(df), df$Y)

init_theta &lt;- as.matrix(c(0))
grad_cost(X1, Y1, init_theta)
</code></pre>

<pre><code>[1] 5321
</code></pre>

<pre><code class="r">
iterations = 100
alpha = 0.1
results &lt;- gradDescent(X1, Y1, init_theta, iterations, alpha)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code>## Error: object &#39;cost.df&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">grad_cost(X1, Y1, theta[[1]])
</code></pre>

<pre><code>[1] 351.4
</code></pre>

<pre><code class="r">## Make some predictions
intercept &lt;- df[df$X == 0, ]$Y
pred &lt;- function(x) return(intercept + c(x) %*% theta)
new_points &lt;- c(0.1, 0.5, 0.8, 1.1)
new_preds &lt;- data.frame(X = new_points, Y = sapply(new_points, pred))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">ggplot(data = df, aes(x = X, y = Y)) + geom_point(size = 2)
</code></pre>

<p><img src="figure/new_point1.png" alt="plot of chunk new_point"> </p>

<pre><code class="r">ggplot(data = df, aes(x = X, y = Y)) + geom_point() + geom_point(data = new_preds, 
    aes(x = X, y = Y, color = &quot;red&quot;), size = 3) + scale_colour_discrete(guide = FALSE)
</code></pre>

<p><img src="figure/new_point2.png" alt="plot of chunk new_point"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent - summary</h1>

<p><space></p>

<ul>
<li>minimization algorithm</li>
<li>approximation, non-closed form solution</li>
<li>good for large number of examples</li>
<li>hard to select the right \(\alpha\)</li>
<li>traditional looping is slow - optimization algorithms are used in practice</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>very popular classification algorithm</li>
<li>based on Binomial error terms, i.e. 1&#39;s and 0&#39;s</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>used widely in modern data analysis</li>
<li>not well understood</li>
<li>intuition: reduce data into only relevant dimensions</li>
<li>the goal of PCA is to compute the most meaningful was to re-express noisy data, revealing the hidden structure</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>first big assumption: linearity</li>
<li>\(PX=Y\)

<ul>
<li>\(X\) is original dataset, \(P\) is a transformation of \(X\) into \(Y\)</li>
</ul></li>
<li>how do we choose \(P\)?

<ul>
<li>reduce noise</li>
<li>maximize variance</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li><p>covariance matrix</p>

<ul>
<li>\(C = X*X^{T}\)</li>
</ul></li>
<li><p>restated goals are</p>

<ul>
<li>minimize covariance and maximize variance</li>
<li>the optimizal \(C\) is a diagonal matrix, off diagonals are = 0</li>
</ul></li>
</ul>

<hr>

<h2>Principle Component Analysis</h2>

<h1>Concepts</h1>

<p><space></p>

<ul>
<li>summary of assumptions

<ul>
<li>linearity (non-linear is a kernel PCA)</li>
<li>largest variance indicates most signal, low variance = noise</li>
<li>orthogonal components - makes the linear algebra easier</li>
<li>assumes data is normally distributed, otherwise PCA might not diagonalize matrix</li>
<li>can use ICA</li>
<li>but most data is normal and PCA is robust to slight deviance from normality</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>\(Ax = \lambdax\)

<ul>
<li>\(\lambda\) is an eigenvalue of \(A\) and \(x\) is an eigenvector of \(A\)</li>
</ul></li>
<li>\(Ax - \lambdaIx = 0\)</li>
<li>\((A - \lambdaI)x = 0\)</li>
<li>\(\det(A - \lambdaI)\) = 0</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(\[A=\left[{\begin{array}{cc}5 & 2 \\2 & 5\\\end{array}\right ]\]\)</p>

<p>A = matrix(c(5,2,2,5),nrow=2)
I = diag(nrow(A))
|A - L<em>I| = 0
det(c(5-l,2,2,5-l))
(5-l)</em>(5-l) - 4 = 0
25 - 10l + l<sup>2</sup> - 4 = 0
l<sup>2</sup> - 10l + 21 = 0
roots &lt;- Re(polyroot(c(21,-10,1)))</p>

<pre><code></code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when lambda = -3
Ax = 3x
5x1 + 2x2 = 3x1
2x1 + 5x2 = 3x2
x1=-x2</li>
<li>one eigenvector = [1 -1]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when lambda = 7
5x1 + 2x2 = 7x1
2x2 + 5x2 = 7x2
x1 = x2</li>
<li>another eigenvector = [1 1]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>A%<em>%c(1,-1) == 3 * as.matrix(c(1,-1))
A%</em>%c(1,1) == 7 * as.matrix(c(1,1))
roots</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>check
m &lt;- matrix(c(1,-1,1,1),ncol=2)
m &lt;- m/sqrt(norm(m))
as.matrix(m%<em>%diag(roots)%</em>%t(m))</li>
<li>lambda is a diagonal matrix, with 0 off diagonals</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>PX = Y</p>

<p>CY = (1/(n-1))*YYt
=PX(PX)t
=PXXtPt
=PAPt</p>

<h1>P is a matrix with columns that are eigenvectors</h1>

<h1>A is a diagonalized matrix of eigenvalues (by linear algebra) and symmetric</h1>

<p>A = EDEt</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>each row of P should be an eigenvector of A</h1>

<p>P=Et</p>

<h1>also note that Pt = P-1 (linear algebra)</h1>

<p>A = PtDP
CY = PPtDPPt
= (1/(n-1))*D</p>

<h1>D is a diagonal matrix, depending on how we choose P</h1>

<h1>therefore CY is diagonalized</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">data &lt;- read.csv(&quot;tennis_data_2013.csv&quot;)
</code></pre>

<pre><code>## Warning: cannot open file &#39;tennis_data_2013.csv&#39;: No such file or
## directory
</code></pre>

<pre><code>## Error: cannot open the connection
</code></pre>

<pre><code class="r">data$Player1 &lt;- as.character(data$Player1)
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

<pre><code class="r">data$Player2 &lt;- as.character(data$Player2)
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

<pre><code class="r">
tennis &lt;- data
m &lt;- length(data)

for (i in 10:m) {
    tennis[, i] &lt;- ifelse(is.na(data[, i]), 0, data[, i])
}
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

<pre><code class="r">
str(tennis)
</code></pre>

<pre><code>## function (..., list = character(), package = NULL, lib.loc = NULL, 
##     verbose = getOption(&quot;verbose&quot;), envir = .GlobalEnv)
</code></pre>

<pre><code class="r">
features &lt;- tennis[, 10:m]
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

<pre><code class="r">
head(features)
</code></pre>

<pre><code>##   classification developer domain coupon plan_period plan_type
## 1              0         0      1      1      Annual  Standard
## 2              1         0      1      0      Annual  Business
## 3              0         0      1      0       Month Unlimited
## 4              1         0      1      1      Annual  Business
## 5              1         0      1      0      Annual Unlimited
## 6              0         0      1      0      Annual Unlimited
##   num_prod_sold num_prod_inventory sold_over_inventory     GMV num_starred
## 1             0                  5               0.000       0           0
## 2           300                220               1.364 1421525           0
## 3             8                  8               1.000   18000           0
## 4          1207               1065               1.133 6957645           0
## 5           242                220               1.100 1672000           0
## 6             0                  2               0.000       0           0
##   num_on_sale avg_prod_price avg_purchase_amt unique_cust discounts_given
## 1           0          13100                0           0               0
## 2           0           6920             6461           4           27445
## 3           0           3375             2250           2               0
## 4           0           6542             6533          12            5405
## 5           0           7182             7600           3               0
## 6           0           8000                0           0               0
##   num_physical num_digital num_service percent_physical percent_digital
## 1            0           1           0                0               1
## 2           63           0           0                1               0
## 3            0           0           3                0               0
## 4           17           0           0                1               0
## 5           11           0           0                1               0
## 6            0           0           0                0               0
##   percent_service num_with_shipping_cost num_with_taxes views_30day
## 1               0                      0              0         169
## 2               0                      0              0        2279
## 3               1                      0              0          56
## 4               0                     17              0         584
## 5               0                      0              0        1037
## 6               0                      0              0         110
##   visits_30day views_7day visits_7day
## 1           81         63          19
## 2          453        402          94
## 3           20         27           6
## 4          191        153          53
## 5          363        341         114
## 6           29         47          12
</code></pre>

<pre><code class="r">str(features)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    299 obs. of  28 variables:
##  $ classification        : int  0 1 0 1 1 0 0 0 1 0 ...
##  $ developer             : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ domain                : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ coupon                : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 1 1 1 1 1 2 ...
##  $ plan_period           : Factor w/ 2 levels &quot;Annual&quot;,&quot;Month&quot;: 1 1 2 1 1 1 1 1 2 1 ...
##  $ plan_type             : Factor w/ 3 levels &quot;Business&quot;,&quot;Standard&quot;,..: 2 1 3 1 3 3 2 2 3 1 ...
##  $ num_prod_sold         : int  0 300 8 1207 242 0 508 0 6916 0 ...
##  $ num_prod_inventory    : int  5 220 8 1065 220 2 280 1 3666 0 ...
##  $ sold_over_inventory   : num  0 1.36 1 1.13 1.1 ...
##  $ GMV                   : int  0 1421525 18000 6957645 1672000 0 802800 0 45825286 0 ...
##  $ num_starred           : int  0 0 0 0 0 0 0 0 1540 0 ...
##  $ num_on_sale           : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ avg_prod_price        : num  13100 6920 3375 6542 7182 ...
##  $ avg_purchase_amt      : num  0 6461 2250 6533 7600 ...
##  $ unique_cust           : int  0 4 2 12 3 0 21 0 1 0 ...
##  $ discounts_given       : int  0 27445 0 5405 0 0 0 0 26784 0 ...
##  $ num_physical          : int  0 63 0 17 11 0 70 0 244 0 ...
##  $ num_digital           : int  1 0 0 0 0 0 0 0 0 0 ...
##  $ num_service           : int  0 0 3 0 0 0 3 0 0 0 ...
##  $ percent_physical      : num  0 1 0 1 1 ...
##  $ percent_digital       : num  1 0 0 0 0 0 0 0 0 0 ...
##  $ percent_service       : num  0 0 1 0 0 ...
##  $ num_with_shipping_cost: int  0 0 0 17 0 0 0 0 244 0 ...
##  $ num_with_taxes        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ views_30day           : int  169 2279 56 584 1037 110 179 28 10966 192 ...
##  $ visits_30day          : int  81 453 20 191 363 29 63 14 3385 116 ...
##  $ views_7day            : int  63 402 27 153 341 47 72 5 2238 33 ...
##  $ visits_7day           : int  19 94 6 53 114 12 25 3 713 13 ...
</code></pre>

<pre><code class="r">dim(features)
</code></pre>

<pre><code>## [1] 299  28
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">scaled_features &lt;- as.matrix(scale(features))
</code></pre>

<pre><code>## Error: &#39;x&#39; must be numeric
</code></pre>

<pre><code class="r">Cx &lt;- cov(scaled_features)
</code></pre>

<pre><code>## Error: object &#39;scaled_features&#39; not found
</code></pre>

<pre><code class="r">eigenvalues &lt;- eigen(Cx)$values
</code></pre>

<pre><code>## Error: object &#39;Cx&#39; not found
</code></pre>

<pre><code class="r">eigenvectors &lt;- eigen(Cx)$vectors
</code></pre>

<pre><code>## Error: object &#39;Cx&#39; not found
</code></pre>

<pre><code class="r">PC &lt;- scaled_features %*% eigenvectors
</code></pre>

<pre><code>## Error: object &#39;scaled_features&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">Cy &lt;- cov(PC)
</code></pre>

<pre><code>## Error: object &#39;PC&#39; not found
</code></pre>

<pre><code class="r">sum(round(diag(Cy) - eigenvalues, 5))
</code></pre>

<pre><code>## Error: object &#39;Cy&#39; not found
</code></pre>

<pre><code class="r">sum(round(Cy[upper.tri(Cy)], 5))  ## off diagonals are 0 since PC&#39;s are orthogonal
</code></pre>

<pre><code>## Error: object &#39;Cy&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">var_explained &lt;- round(eigenvalues/sum(eigenvalues) * 100, digits = 2)
</code></pre>

<pre><code>## Error: object &#39;eigenvalues&#39; not found
</code></pre>

<pre><code class="r">cum_var_explained &lt;- round(cumsum(eigenvalues)/sum(eigenvalues) * 100, digits = 2)
</code></pre>

<pre><code>## Error: object &#39;eigenvalues&#39; not found
</code></pre>

<pre><code class="r">
var_explained &lt;- as.data.frame(var_explained)
</code></pre>

<pre><code>## Error: object &#39;var_explained&#39; not found
</code></pre>

<pre><code class="r">names(var_explained) &lt;- &quot;variance_explained&quot;
</code></pre>

<pre><code>## Error: object &#39;var_explained&#39; not found
</code></pre>

<pre><code class="r">var_explained$PC &lt;- as.numeric(rownames(var_explained))
</code></pre>

<pre><code>## Error: object &#39;var_explained&#39; not found
</code></pre>

<pre><code class="r">var_explained &lt;- cbind(var_explained, cum_var_explained)
</code></pre>

<pre><code>## Error: object &#39;var_explained&#39; not found
</code></pre>

<pre><code class="r">
library(ggplot2)
ggplot(var_explained) + geom_bar(aes(x = PC, y = variance_explained), stat = &quot;identity&quot;) + 
    geom_line(aes(x = PC, y = cum_var_explained))
</code></pre>

<pre><code>## Error: object &#39;var_explained&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">pca.df &lt;- prcomp(scaled_features)
</code></pre>

<pre><code>## Error: object &#39;scaled_features&#39; not found
</code></pre>

<pre><code class="r">eigenvalues == (pca.df$sdev)^2
</code></pre>

<pre><code>## Error: object &#39;eigenvalues&#39; not found
</code></pre>

<pre><code class="r">eigenvectors[, 1] == pca.df$rotation[, 1]
</code></pre>

<pre><code>## Error: object &#39;eigenvectors&#39; not found
</code></pre>

<pre><code class="r">sum((eigenvectors[, 1])^2)
</code></pre>

<pre><code>## Error: object &#39;eigenvectors&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">rows &lt;- nrow(tennis)
pca.plot &lt;- as.data.frame(pca.df$x[, 1:2])
</code></pre>

<pre><code>## Error: object &#39;pca.df&#39; not found
</code></pre>

<pre><code class="r">pca.plot$gender &lt;- data$Gender
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

<pre><code class="r">ggplot(data = pca.plot, aes(x = PC1, y = PC2, color = gender)) + geom_point()
</code></pre>

<pre><code>## Error: object &#39;pca.plot&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>how accurate is the first PC at dividing the dataset?
gen &lt;- ifelse(pca.df$x[,1] &gt; abs(mean(pca.df$x[,1]))*2,&quot;F&quot;,&quot;M&quot;)
sum(diag(table(gen,as.character(data$Gender))))/rows</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>separate data into meaningful or useful groups

<ul>
<li>capture natural structure of the data</li>
<li>starting point for further analysis</li>
</ul></li>
<li>cluster for utility

<ul>
<li>summarizing data for less expensive computation</li>
<li>data compression</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Types of Clusters</h1>

<p><space></p>

<ul>
<li>data that looks more like other data in that cluster than outside</li>
<li>each data point is more similar to the prototype (centeroid) of the cluster than the prototype of other clusters</li>
<li>where the density is highest, that is a cluster</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Typical clustering problem</h1>

<p><space></p>

<p><img src="figure/cluster_plot_example.png" alt="plot of chunk cluster_plot_example"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Density based cluster</h1>

<p><space></p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/0/05/DBSCAN-density-data.svg" density_based /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>prototype, partitional based</li>
<li>choose K initial centroids/clusters</li>
<li>points are assigned to the closest centroid</li>
<li>centroid is then updated based on the points in that cluster</li>
<li>update steps until no point changes or centroids remain the same</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ol>
<li>Select K points as initial centroids. </li>
<li>Repeat</li>
<li>&nbsp;&nbsp; Form K clusters by assigning each point to its closest centroid.</li>
<li>&nbsp;&nbsp; Recompute the centroid of each cluster. </li>
<li>until Centroids do not change, or change very minimally, i.e. &lt;1%</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ul>
<li>Use similarity measures (Euclidean or cosine) depending on the data</li>
<li>Minimize the squared distance of each point to closest centroid

<ul>
<li>minimize the objective function</li>
<li>the centroid that minimizes the SSE of the cluster is the mean</li>
<li>leads to local minimum - not global - since optimizing based on chosen centroids</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li><p>choose K randomly - can lead to poor centroids</p>

<ul>
<li>run k-means multiple times - still doesnâ€™t solve problems</li>
</ul></li>
<li><p>can reduce the total SSE by increasing K</p>

<ul>
<li>can increase the cluster with largest SSE</li>
</ul></li>
<li><p>can decrease K and minimize SSE</p>

<ul>
<li>split up a cluster into other clusters</li>
<li>the centroid that is split will increase total SSE the least</li>
</ul></li>
<li><p>bisecting K means</p>

<ul>
<li>less susceptible to initialization problems</li>
<li>split points into 2 clusters

<ul>
<li>take cluster with largest SSE - split that into two clusters</li>
</ul></li>
<li>rerun bisecting K mean on resulting clusters</li>
<li>stop when you have K clusters</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="C:/Users/Ilan%20Man/Desktop/Personal/RPres_ML_2/figure/different_density.png" alt="different_density"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="/RPres_ML_2/figure/different_size_clusters.png" alt="different_size_clusters"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="/RPres_ML_2/figure/non-globular.png" alt="non-globular"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<pre><code class="r">tennis_kmean &lt;- kmeans(features, centers = 5)
</code></pre>

<pre><code>## Warning: NAs introduced by coercion
</code></pre>

<pre><code>## Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<pre><code class="r">table(tennis$Gender, tennis_kmean$cluster)
</code></pre>

<pre><code>## Error: object of type &#39;closure&#39; is not subsettable
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans animation</h1>

<p><space></p>

<p>install.packages(&#39;animation&#39;)
library(animation)</p>

<p>oopt = ani.options(interval = 1)
ani_ex = rbind(matrix(rnorm(100, sd = 0.3), ncol = 2), 
          matrix(rnorm(100, sd = 0.3), 
          ncol = 2))
colnames(ani_ex) = c(&quot;x&quot;, &quot;y&quot;)</p>

<p>kmeans.an = function(
  x = cbind(X1 = runif(50), X2 = runif(50)), centers = 4, hints = c(&#39;Move centers!&#39;, &#39;Find cluster?&#39;),
  pch = 1:5, col = 1:5
) {
  x = as.matrix(x)
  ocluster = sample(centers, nrow(x), replace = TRUE)
  if (length(centers) == 1) centers = x[sample(nrow(x), centers), ] else
    centers = as.matrix(centers)
  numcent = nrow(centers)
  dst = matrix(nrow = nrow(x), ncol = numcent)
  j = 1
  pch = rep(pch, length = numcent)
  col = rep(col, length = numcent)</p>

<p>for (j in 1:ani.options(&#39;nmax&#39;)) {
    dev.hold()
    plot(x, pch = pch[ocluster], col = col[ocluster], panel.first = grid())
    mtext(hints[1], 4)
    points(centers, pch = pch[1:numcent], cex = 3, lwd = 2, col = col[1:numcent])
    ani.pause()
    for (i in 1:numcent) {
      dst[, i] = sqrt(apply((t(t(x) - unlist(centers[i, ])))<sup>2,</sup> 1, sum))
    }
    ncluster = apply(dst, 1, which.min)
    plot(x, type = &#39;n&#39;)
    mtext(hints[2], 4)
    grid()
    ocenters = centers
    for (i in 1:numcent) {
      xx = subset(x, ncluster == i)
      polygon(xx[chull(xx), ], density = 10, col = col[i], lty = 2)
      points(xx, pch = pch[i], col = col[i])
      centers[i, ] = apply(xx, 2, mean)
    }
    points(ocenters, cex = 3, col = col[1:numcent], pch = pch[1:numcent], lwd = 2)
    ani.pause()
    if (all(ncluster == ocluster)) break
    ocluster = ncluster
  }
  invisible(list(cluster = ncluster, centers = centers))
}</p>

<p>kmeans.an(ani_ex, centers = 5, hints = c(&quot;Move centers&quot;,&quot;Cluster found?&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<ul>
<li>density based

<ul>
<li>center based approach to finding density</li>
<li>count the number of points within some radius of a point, the radius is call Eps</li>
<li>if Eps is too big, there will be m points, if eps is too small, there will be 1 point</li>
<li>core point has X points within a radius of Eps, border points are within a radius of Eps of core point, and noise points are not within Eps of border or core points</li>
<li>if p is density connected to q, they are part of the same cluster, if not, then they are not; if p is not density connected to any other point, its considered noise</li>
</ul></li>
</ul>

<hr>

<h2>Clustering</h2>

<h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">x &lt;- c(2, 2, 8, 5, 7, 6, 1, 4)
y &lt;- c(10, 5, 4, 8, 5, 4, 2, 9)
cluster &lt;- data.frame(X = c(x, 2 * x, 3 * x), Y = c(y, -2 * x, 1/4 * y))
plot(cluster)
</code></pre>

<p><img src="figure/unnamed-chunk-8.png" alt="plot of chunk unnamed-chunk-8"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">library(fpc)
cluster_DBSCAN &lt;- dbscan(cluster, eps = 3, MinPts = 2, method = &quot;hybrid&quot;)
plot(cluster_DBSCAN, cluster, main = &quot;Clustering using DBSCAN algorithm (eps=3, MinPts=3)&quot;)
</code></pre>

<p><img src="figure/dbscan_ex.png" alt="plot of chunk dbscan_ex"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>representation of decisions made in order to classify or predict
<img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/tree_example.png" alt="overview"></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/tree_structure.png" alt="structure"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<ul>
<li>recursive partitioning -&gt; &quot;divide and conquer&quot;</li>
<li>going down, choose feature that is most <em>predictive</em> of target class

<ul>
<li>split the data according to feature</li>
<li>continue...</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p>until...</p>

<ul>
<li>all examples at a node are in same class</li>
<li>no more features left to distinguish (prone to overfitting)</li>
<li>tree has grown to some prespecified limit (prune)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Algorithms</h1>

<p><space></p>

<ul>
<li>ID3

<ul>
<li>original, popular, DT implementation</li>
</ul></li>
<li>C4.5

<ul>
<li>like ID3 +</li>
<li>handles continuous cases</li>
<li>imputing missing values</li>
<li>weighing costs</li>
<li>pruning post creation</li>
</ul></li>
<li>C5.0

<ul>
<li>like C4.5 + </li>
<li>faster, less memory usage</li>
<li>boosting</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Selecting features</h1>

<p><space></p>

<ul>
<li>How does tree decide how to select feature?

<ul>
<li>purity of resulting split</li>
</ul></li>
<li><strong>Entropy</strong>: amount of information contained in a random variable

<ul>
<li>For a feature with N classes:</li>
<li>0 = purely homogenous</li>
<li>\(\log_{2}(N)\) = completely mixed</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<p>\(Entropy(S) = \sum_{i=1}^{c} -p_{i}\log_{2}(p_{i})\)</p>

<ul>
<li>where \(S\) is a dataset</li>
<li>\(c\) is the number of levels in that data</li>
<li>\(p_{i}\) is the proportion of values in that level</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a fair, 6 sided die?</p>

<pre><code class="r">entropy &lt;- function(probs) {
    ent &lt;- 0
    for (i in probs) {
        ent_temp &lt;- -i * log2(i)
        ent &lt;- ent + ent_temp
    }
    return(ent)
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">fair &lt;- rep(1/6, 6)
entropy(fair)
</code></pre>

<pre><code>## [1] 2.585
</code></pre>

<pre><code class="r">log2(6)
</code></pre>

<pre><code>## [1] 2.585
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a biased, 6 sided die?</p>

<ul>
<li>\(P(X=1) = P(X=2) = P(X=3) = 1/9\)</li>
<li>\(P(X=4) = P(X=5) = P(X=6) = 2/9\)</li>
</ul>

<pre><code class="r">biased &lt;- c(rep(1/9, 3), rep(2/9, 3))
entropy(biased)
</code></pre>

<pre><code>[1] 2.503
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">more_biased &lt;- c(rep(1/18, 3), rep(5/18, 3))
entropy(more_biased)
</code></pre>

<pre><code>[1] 2.235
</code></pre>

<pre><code class="r">
most_biased &lt;- c(rep(1/100, 5), rep(95/100, 1))
entropy(most_biased)
</code></pre>

<pre><code>[1] 0.4025
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">curve(-x * log2(x) - (1 - x) * log2(1 - x), col = &quot; red&quot;, xlab = &quot;x&quot;, ylab = &quot;Entropy&quot;, 
    lwd = 4, main = &quot;Entropy of a coin toss&quot;)
</code></pre>

<p><img src="figure/entropy_curve.png" alt="plot of chunk entropy_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<ul>
<li>C5.0 uses the change in entropy to determine the change in purity</li>
<li><p>InfoGain = Entropy (pre split) - Entropy (post split)</p>

<ul>
<li>Entropy (pre split) = current Entropy</li>
<li>Entropy (post split) is trickier</li>
<li>need to consider Entropy of each possible split</li>
<li>\(E(post) = \sum_{i=1}^{n}w_{i}Entropy(P_{i})\)</li>
</ul></li>
<li><p>Notes:</p>

<ul>
<li>The more a feature splits the data in obvious ways, the less informative it is, entropy is lower</li>
<li>The more a feature splits the data - in general - the higher the entropy and hence information gained by splitting at that feature</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">voting_data &lt;- read.csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&quot;)
names(voting_data) &lt;- c(&quot;party&quot;, &quot;handicapped-infants&quot;, &quot;water-project-cost-sharing&quot;, 
    &quot;adoption-of-the-budget-resolution&quot;, &quot;physician-fee-freeze&quot;, &quot;el-salvador-aid&quot;, 
    &quot;religious-groups-in-schools&quot;, &quot;anti-satellite-test-ban&quot;, &quot;aid-to-nicaraguan-contras&quot;, 
    &quot;mx-missile&quot;, &quot;immigration&quot;, &quot;synfuels-corporation-cutback&quot;, &quot;education-spending&quot;, 
    &quot;superfund-right-to-sue&quot;, &quot;crime&quot;, &quot;duty-free-exports&quot;, &quot;export-administration-act-south-africa&quot;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">prop.table(table(voting_data[, 1]))
</code></pre>

<pre><code>
  democrat republican 
    0.6152     0.3848 
</code></pre>

<pre><code class="r">n &lt;- nrow(voting_data)
train_ind &lt;- sample(n, 2/3 * n)
voting_train &lt;- voting_data[train_ind, ]
voting_test &lt;- voting_data[-train_ind, ]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/real_tree_example.png" height="500px" width="500px" /></p>

<pre><code>## Error: object &#39;voting_train&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code>

   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|


Total Observations in Table:  145 


             | predicted class 
actual class |   democrat | republican |  Row Total | 
-------------|------------|------------|------------|
    democrat |         92 |          4 |         96 | 
             |      0.634 |      0.028 |            | 
-------------|------------|------------|------------|
  republican |          4 |         45 |         49 | 
             |      0.028 |      0.310 |            | 
-------------|------------|------------|------------|
Column Total |         96 |         49 |        145 | 
-------------|------------|------------|------------|


</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># most important variables
head(C5imp(tree_model))
</code></pre>

<pre><code>##                                   Overall
## physician-fee-freeze                97.23
## adoption-of-the-budget-resolution   43.60
## el-salvador-aid                     35.64
## anti-satellite-test-ban             10.03
## synfuels-corporation-cutback         4.50
## handicapped-infants                  0.00
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># in-sample error rate
summary(tree_model)
</code></pre>

<pre><code>## 
## Call:
## C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 1)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Thu Aug 14 20:51:32 2014
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 289 cases (17 attributes) from undefined.data
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (162.5/2.1)
## physician-fee-freeze = y:
## :...adoption-of-the-budget-resolution = ?: republican (0)
##     adoption-of-the-budget-resolution = n:
##     :...el-salvador-aid = n: democrat (1.7/0.4)
##     :   el-salvador-aid in {?,y}: republican (101.5/2.4)
##     adoption-of-the-budget-resolution = y:
##     :...anti-satellite-test-ban in {?,y}: republican (13.6/0.5)
##         anti-satellite-test-ban = n:
##         :...synfuels-corporation-cutback = n: republican (4.5/1.2)
##             synfuels-corporation-cutback in {?,y}: democrat (5.2)
## 
## 
## Evaluation on training data (289 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       6    6( 2.1%)   &lt;&lt;
## 
## 
##     (a)   (b)    &lt;-classified as
##    ----  ----
##     168     3    (a): class democrat
##       3   115    (b): class republican
## 
## 
##  Attribute usage:
## 
##   97.23% physician-fee-freeze
##   43.60% adoption-of-the-budget-resolution
##   35.64% el-salvador-aid
##   10.03% anti-satellite-test-ban
##    4.50% synfuels-corporation-cutback
## 
## 
## Time: 0.0 secs
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Boosting</h1>

<p><space></p>

<ul>
<li>by combining a number of weak performing learners create a team that is much stronger than any one of the learners alone.</li>
<li>this is where C5.0 improves on C4.5</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<pre><code class="r">boosted_tree_model &lt;- C5.0(voting_train[, -1], voting_train[, 1], trials = 25)
boosted_tennis_predict &lt;- predict(boosted_tree_model, voting_test[, -1])

boosted_conf &lt;- CrossTable(voting_test[, 1], boosted_tennis_predict, prop.chisq = FALSE, 
    prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         93 |          3 |         96 | 
##              |      0.641 |      0.021 |            | 
## -------------|------------|------------|------------|
##   republican |          2 |         47 |         49 | 
##              |      0.014 |      0.324 |            | 
## -------------|------------|------------|------------|
## Column Total |         95 |         50 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<pre><code class="r"># in-sample error rate
summary(boosted_tree_model)
</code></pre>

<pre><code>## 
## Call:
## C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 25)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Thu Aug 14 20:51:33 2014
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 289 cases (17 attributes) from undefined.data
## 
## -----  Trial 0:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (162.5/2.1)
## physician-fee-freeze = y:
## :...adoption-of-the-budget-resolution = ?: republican (0)
##     adoption-of-the-budget-resolution = n:
##     :...el-salvador-aid = n: democrat (1.7/0.4)
##     :   el-salvador-aid in {?,y}: republican (101.5/2.4)
##     adoption-of-the-budget-resolution = y:
##     :...anti-satellite-test-ban in {?,y}: republican (13.6/0.5)
##         anti-satellite-test-ban = n:
##         :...synfuels-corporation-cutback = n: republican (4.5/1.2)
##             synfuels-corporation-cutback in {?,y}: democrat (5.2)
## 
## -----  Trial 1:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution in {?,y}: democrat (154.8/25.9)
## adoption-of-the-budget-resolution = n:
## :...duty-free-exports in {?,n}: republican (111.6/17.8)
##     duty-free-exports = y: democrat (22.5/4.7)
## 
## -----  Trial 2:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (128.6/22.6)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (74.1/2.5)
##     immigration = n:
##     :...duty-free-exports in {?,n}: republican (59.2/17.7)
##         duty-free-exports = y: democrat (27.1/6.3)
## 
## -----  Trial 3:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (116.4/27.2)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (108.1/19.5)
##     synfuels-corporation-cutback = y: democrat (64.5/24)
## 
## -----  Trial 4:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,y}: republican (189.6/56.1)
## physician-fee-freeze = n:
## :...adoption-of-the-budget-resolution = n: republican (28.9/10.7)
##     adoption-of-the-budget-resolution in {?,y}: democrat (70.4/8)
## 
## -----  Trial 5:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (106/22.4)
## physician-fee-freeze = y:
## :...education-spending in {?,y}: republican (115/30)
##     education-spending = n:
##     :...anti-satellite-test-ban in {?,n}: democrat (43.3/10.2)
##         anti-satellite-test-ban = y: republican (24.7/9.9)
## 
## -----  Trial 6:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (92.8/24)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (87.6/21.3)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,y}: democrat (21.9/3.7)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (12.3)
##             anti-satellite-test-ban = n:
##             :...adoption-of-the-budget-resolution = n: republican (50.8/18.9)
##                 adoption-of-the-budget-resolution in {?,y}: democrat (23.6/1.5)
## 
## -----  Trial 7:  -----
## 
## Decision tree:
## 
## duty-free-exports = ?: democrat (0)
## duty-free-exports = y:
## :...crime in {?,n}: democrat (28.4/0.8)
## :   crime = y:
## :   :...anti-satellite-test-ban in {?,n}: democrat (53.9/12.6)
## :       anti-satellite-test-ban = y: republican (15.9/2.8)
## duty-free-exports = n:
## :...synfuels-corporation-cutback in {?,n}: republican (79.6/11)
##     synfuels-corporation-cutback = y:
##     :...physician-fee-freeze in {?,n}: democrat (15.2/1.3)
##         physician-fee-freeze = y:
##         :...immigration in {?,n}: democrat (57.7/17.4)
##             immigration = y: republican (38.4/11.8)
## 
## -----  Trial 8:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution = ?: democrat (0)
## adoption-of-the-budget-resolution = n:
## :...immigration in {?,y}: republican (48.4/7.3)
## :   immigration = n:
## :   :...superfund-right-to-sue = n: democrat (15.5/1.5)
## :       superfund-right-to-sue in {?,y}: republican (77.7/35)
## adoption-of-the-budget-resolution = y:
## :...physician-fee-freeze in {?,n}: democrat (77.1/6.8)
##     physician-fee-freeze = y:
##     :...anti-satellite-test-ban in {?,n}: democrat (49.7/13.7)
##         anti-satellite-test-ban = y: republican (20.7/1.5)
## 
## -----  Trial 9:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (114/25.6)
## physician-fee-freeze = y:
## :...handicapped-infants in {?,y}: republican (41.6/7.2)
##     handicapped-infants = n:
##     :...superfund-right-to-sue in {?,n}: republican (15.9/1.7)
##         superfund-right-to-sue = y:
##         :...crime = ?: democrat (0)
##             crime = n: republican (4.9)
##             crime = y:
##             :...mx-missile in {?,y}: democrat (22.2/3.5)
##                 mx-missile = n:
##                 :...anti-satellite-test-ban in {?,y}: republican (8.1)
##                     anti-satellite-test-ban = n:
##                     :...synfuels-corporation-cutback = n: republican (33/9.8)
##                         synfuels-corporation-cutback in {?,
##                                                          y}: democrat (49.4/15.7)
## 
## -----  Trial 10:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (100.5/26.2)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (78.6/15.7)
##     immigration = n:
##     :...duty-free-exports in {?,n}: republican (79.9/30.9)
##         duty-free-exports = y: democrat (30.1/7.3)
## 
## -----  Trial 11:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution = ?: democrat (0)
## adoption-of-the-budget-resolution = y:
## :...physician-fee-freeze in {?,n}: democrat (47.6/4.8)
## :   physician-fee-freeze = y:
## :   :...synfuels-corporation-cutback = n: republican (35.7/9.6)
## :       synfuels-corporation-cutback in {?,y}: democrat (41.4/8.5)
## adoption-of-the-budget-resolution = n:
## :...immigration in {?,y}: republican (62.1/13.6)
##     immigration = n:
##     :...physician-fee-freeze in {?,n}: democrat (12.8/1.6)
##         physician-fee-freeze = y:
##         :...education-spending = n: democrat (30/9.1)
##             education-spending in {?,y}: republican (59.5/20.4)
## 
## -----  Trial 12:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (89.4/25)
## physician-fee-freeze = y:
## :...export-administration-act-south-africa = n: democrat (68.2/27.9)
##     export-administration-act-south-africa in {?,y}: republican (131.4/44.4)
## 
## -----  Trial 13:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (83.6/24.9)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (86.6/22.6)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,y}: democrat (28.1/7.5)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (12.6)
##             anti-satellite-test-ban = n:
##             :...adoption-of-the-budget-resolution = n: republican (57.2/21.7)
##                 adoption-of-the-budget-resolution in {?,y}: democrat (20.8/1.6)
## 
## -----  Trial 14:  -----
## 
## Decision tree:
## 
## duty-free-exports in {?,y}: democrat (85.8/22)
## duty-free-exports = n:
## :...synfuels-corporation-cutback = ?: republican (0)
##     synfuels-corporation-cutback = n:
##     :...adoption-of-the-budget-resolution in {?,n}: republican (63.5/4.4)
##     :   adoption-of-the-budget-resolution = y: democrat (21.2/8.3)
##     synfuels-corporation-cutback = y:
##     :...physician-fee-freeze in {?,n}: democrat (10.9/0.7)
##         physician-fee-freeze = y:
##         :...immigration in {?,n}: democrat (61.4/18.5)
##             immigration = y: republican (46.1/14.7)
## 
## -----  Trial 15:  -----
## 
## Decision tree:
## 
## crime = ?: republican (0)
## crime = n: democrat (29.4/7.8)
## crime = y:
## :...religious-groups-in-schools in {?,n}: republican (57.5/8.5)
##     religious-groups-in-schools = y:
##     :...physician-fee-freeze in {?,n}: democrat (20/1.5)
##         physician-fee-freeze = y:
##         :...handicapped-infants in {?,y}: republican (25/1.1)
##             handicapped-infants = n:
##             :...education-spending in {?,n}: democrat (60.7/16.6)
##                 education-spending = y:
##                 :...synfuels-corporation-cutback in {?,
##                     :                                n}: republican (25.6)
##                     synfuels-corporation-cutback = y:
##                     :...anti-satellite-test-ban in {?,n}: democrat (61.7/23.9)
##                         anti-satellite-test-ban = y: republican (9.1)
## 
## -----  Trial 16:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (75.8/22.1)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (90.5/19.5)
##     immigration = n:
##     :...mx-missile = ?: republican (0)
##         mx-missile = y: democrat (27.1/7)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (14.8)
##             anti-satellite-test-ban = n:
##             :...duty-free-exports = ?: republican (0)
##                 duty-free-exports = y: democrat (13.7/1.4)
##                 duty-free-exports = n: [S1]
## 
## SubTree [S1]
## 
## export-administration-act-south-africa = n: democrat (39.1/17.3)
## export-administration-act-south-africa in {?,y}: republican (28/0.9)
## 
## -----  Trial 17:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution = ?: republican (0)
## adoption-of-the-budget-resolution = y:
## :...physician-fee-freeze in {?,n}: democrat (35.3/2.6)
## :   physician-fee-freeze = y:
## :   :...anti-satellite-test-ban = n: democrat (46.8/12.8)
## :       anti-satellite-test-ban in {?,y}: republican (29.8/3)
## adoption-of-the-budget-resolution = n:
## :...duty-free-exports = ?: republican (0)
##     duty-free-exports = y: democrat (22.3/7.8)
##     duty-free-exports = n:
##     :...synfuels-corporation-cutback in {?,n}: republican (72/4.6)
##         synfuels-corporation-cutback = y:
##         :...mx-missile in {?,n}: republican (73.5/21.7)
##             mx-missile = y: democrat (9.3/0.4)
## 
## -----  Trial 18:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (74.5/18.9)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (103.7/20.6)
##     synfuels-corporation-cutback = y:
##     :...mx-missile = ?: republican (0)
##         mx-missile = y: democrat (24.8/6.8)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (10.5)
##             anti-satellite-test-ban = n:
##             :...adoption-of-the-budget-resolution in {?,
##                 :                                     n}: republican (62.2/22.2)
##                 adoption-of-the-budget-resolution = y: democrat (13.3/0.5)
## 
## -----  Trial 19:  -----
## 
## Decision tree:
## 
## immigration in {?,y}: republican (131.7/43.1)
## immigration = n:
## :...physician-fee-freeze in {?,n}: democrat (22.8/1.5)
##     physician-fee-freeze = y:
##     :...education-spending in {?,n}: democrat (58.2/13.9)
##         education-spending = y:
##         :...synfuels-corporation-cutback in {?,n}: republican (23.6)
##             synfuels-corporation-cutback = y: democrat (52.6/23.4)
## 
## -----  Trial 20:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (92.4/23.5)
## physician-fee-freeze = y:
## :...religious-groups-in-schools in {?,n}: republican (29.7/3.6)
##     religious-groups-in-schools = y:
##     :...mx-missile = ?: republican (0)
##         mx-missile = y: democrat (34.5/6.9)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (17.1)
##             anti-satellite-test-ban = n:
##             :...duty-free-exports in {?,n}: republican (90.1/32.9)
##                 duty-free-exports = y: democrat (25.2/7.4)
## 
## -----  Trial 21:  -----
## 
## Decision tree:
## 
## synfuels-corporation-cutback = ?: democrat (0)
## synfuels-corporation-cutback = y:
## :...physician-fee-freeze in {?,n}: democrat (32.9/1)
## :   physician-fee-freeze = y:
## :   :...export-administration-act-south-africa in {?,n}: democrat (55.2/15.7)
## :       export-administration-act-south-africa = y: republican (53.9/22.1)
## synfuels-corporation-cutback = n:
## :...education-spending in {?,y}: republican (53/5.8)
##     education-spending = n:
##     :...crime in {?,n}: democrat (11/0.3)
##         crime = y:
##         :...religious-groups-in-schools in {?,n}: republican (35.4/5.4)
##             religious-groups-in-schools = y: democrat (47.7/15.6)
## 
## -----  Trial 22:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (99.8/24.1)
## physician-fee-freeze = y:
## :...education-spending in {?,y}: republican (91.8/12.3)
##     education-spending = n:
##     :...immigration = n: democrat (58/23)
##         immigration in {?,y}: republican (38.4/11.7)
## 
## -----  Trial 23:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution in {?,n}: republican (143.6/37.3)
## adoption-of-the-budget-resolution = y:
## :...physician-fee-freeze in {?,n}: democrat (48.7/2)
##     physician-fee-freeze = y:
##     :...anti-satellite-test-ban in {?,n}: democrat (59.2/14)
##         anti-satellite-test-ban = y: republican (36.5/4.5)
## 
## -----  Trial 24:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (88.7/4.1)
## physician-fee-freeze = y:
## :...mx-missile = ?: republican (0)
##     mx-missile = y: democrat (61.6/25.8)
##     mx-missile = n:
##     :...water-project-cost-sharing in {?,n}: republican (33)
##         water-project-cost-sharing = y:
##         :...anti-satellite-test-ban in {?,y}: republican (12.6)
##             anti-satellite-test-ban = n:
##             :...duty-free-exports = ?: republican (0)
##                 duty-free-exports = y: democrat (19.9/2)
##                 duty-free-exports = n:
##                 :...adoption-of-the-budget-resolution in {?,
##                     :                                     n}: republican (47.6/1.8)
##                     adoption-of-the-budget-resolution = y: democrat (23.5/6.2)
## 
## 
## Evaluation on training data (289 cases):
## 
## Trial        Decision Tree   
## -----      ----------------  
##    Size      Errors  
## 
##    0      6    6( 2.1%)
##    1      3   30(10.4%)
##    2      4   13( 4.5%)
##    3      3   24( 8.3%)
##    4      3   26( 9.0%)
##    5      4   14( 4.8%)
##    6      6    6( 2.1%)
##    7      7   56(19.4%)
##    8      6   19( 6.6%)
##    9      8   21( 7.3%)
##   10      4   12( 4.2%)
##   11      7   16( 5.5%)
##   12      3   49(17.0%)
##   13      6    5( 1.7%)
##   14      6   30(10.4%)
##   15      8   30(10.4%)
##   16      7   24( 8.3%)
##   17      7   16( 5.5%)
##   18      6    5( 1.7%)
##   19      5   94(32.5%)
##   20      6   16( 5.5%)
##   21      7   30(10.4%)
##   22      4   14( 4.8%)
##   23      4   22( 7.6%)
##   24      7   16( 5.5%)
## boost              3( 1.0%)   &lt;&lt;
## 
## 
##     (a)   (b)    &lt;-classified as
##    ----  ----
##     170     1    (a): class democrat
##       2   116    (b): class republican
## 
## 
##  Attribute usage:
## 
##   98.96% immigration
##   98.27% adoption-of-the-budget-resolution
##   97.23% physician-fee-freeze
##   95.85% crime
##   95.50% synfuels-corporation-cutback
##   93.77% duty-free-exports
##   69.55% education-spending
##   60.55% religious-groups-in-schools
##   50.87% anti-satellite-test-ban
##   46.37% mx-missile
##   43.94% handicapped-infants
##   38.75% superfund-right-to-sue
##   38.06% export-administration-act-south-africa
##   35.99% water-project-cost-sharing
##   35.64% el-salvador-aid
## 
## 
## Time: 0.0 secs
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<ul>
<li>still getting too many false positives (predict republican but actually democrat)</li>
<li>introduce higher cost to getting this wrong</li>
</ul>

<pre><code class="r">error_cost &lt;- matrix(c(0, 1, 2, 0), nrow = 2)
cost_model &lt;- C5.0(voting_train[, -1], voting_train[, 1], trials = 1, costs = error_cost)
</code></pre>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<pre><code class="r">cost_predict &lt;- predict(cost_model, newdata = voting_test[, -1])
conf &lt;- CrossTable(voting_test[, 1], cost_predict, prop.chisq = FALSE, prop.c = FALSE, 
    prop.r = FALSE, dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         89 |          7 |         96 | 
##              |      0.614 |      0.048 |            | 
## -------------|------------|------------|------------|
##   republican |          1 |         48 |         49 | 
##              |      0.007 |      0.331 |            | 
## -------------|------------|------------|------------|
## Column Total |         90 |         55 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<p><img src="figure/plot_boost_acc.png" alt="plot of chunk plot_boost_acc"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Pros and Cons</h1>

<p><space></p>

<ul>
<li>trees are non-parametric, rule based classification or regression method</li>
<li>simple to understand and interpret</li>
<li>little data preparation</li>
<li>works well with small or large number of features
<br></li>
<li>easy to overfit</li>
<li>biased towards splits on features with large number of levels</li>
<li>usually finds local optimum</li>
<li>difficult concepts are hard to learn</li>
<li>avoid pre-pruning</li>
<li>hard to know optimal length of tree without growing it there first</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article>
    <p><space></p>

<ul>
<li><a href="http://www.packtpub.com/machine-learning-with-r/book">Machine Learning with R</a></li>
<li><a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a></li>
<li><a href="http://web.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-84" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>